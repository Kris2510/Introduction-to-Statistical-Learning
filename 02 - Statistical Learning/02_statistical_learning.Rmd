---
title: "An Introduction to Statistical Learning"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 2. Statistical Learning - Exercises


### Conceptual

&nbsp;

#### **1.** For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.


  (a) The sample size n is extremely large, and the number of predictors p is small.  
  *better, with a large sample size the model is less likely to overfit when we increase flexibility*
  (b) The number of predictors p is extremely large, and the number of observations n is small.  
  *worse, with a small sample size a more flexible model will overfit the training data very fast*
  (c) The relationship between the predictors and response is highly non-linear.  
  *better, since less flexible models like linear regression perform poorly on highly non-linear data.*
  (d) The variance of the error terms, i.e.σ2=Var(e), is extremely high.  
  *worse, highly flexible models will follow the noise to closely –> overfitting*

&nbsp;
  
#### **2.** Explain whether each scenario is a classiﬁcation or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.



  (a) We collect a set of data on the top 500 ﬁrms in the US. For each ﬁrm we record proﬁt, number of employees,    industry and the CEO salary. We are interested in understanding which factors aﬀect CEO salary.  
  * n = 500, p= 3, CEO salary is a continous variable so this is a regression problem and since we are interested in understanding which factors influence the CEO salary it is an inference problem
  (b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.  
  * n = 20, p= 13, success or failure -> categorical –> classification , since we want to know if a given product is a success or failure it is a prediction problem.
  (c) We are interest in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.  
  * n = 52, p= 3, since y is continous –> regression, prediction

&nbsp;

#### **3.** We now revisit the bias-variance decomposition.



  (a) Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less ﬂexible statistical learning methods towards more ﬂexible approaches. The x-axis should represent the amount of ﬂexibility in the method, and the y-axis should represent the values for each curve. There should be ﬁve curves. Make sure to label each one.  
 
 ![](bias_variance.png)
 
 (b) Explain why each of the ﬁve curves has the shape displayed in part (a)  
 
       * train-MSE: monotonically decreasing up to a point where it perfectly fits the data  
      
       * test-MSE: declining sharply with increased flexibility but is starting to rise again when the training model starts to overfit the training data (model can’t generalize to new data)  
      
       * Var(e): constant, irreducible error, best possible value test MSE can achieve  
      
       * squared bias: the error that is introduced by explaining a complicated real world problem through a simple model, decreases with increasing flexibility  
  
  * Variance: the amout by which $\hat{f}$ changes if we estimate it using different training data  

&nbsp;

#### **4.** You will now think of some real-life applications for statistical learning.



  (a) Describe three real-life applications in which classiﬁcation might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.
  
      * Image Classification Plants: 3 dimensional RBG matrices, plant name, prediction
      * Predict Gender of a person based on different predictors like salary, education, etc , response = gender, prediction
      * credit card fraud detection, a ton of different predictors, fraud = response, prediction
  
  (b) Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.
  
      * predict success chance of candies, ingredients as well as several other features, response = predicted success chance -> inference
      * which features make a good candy, analyse ingredients and other features to infere which are the most important ones regarding a candies success –> inference
      * predict housing prices in certain areas based on several features such as number of rooms, ocean proximity, lat, long, etc, response = price, prediction  
    
  (c) Describe three real-life applications in which cluster analysis might be useful.
  
      * discover customer segments
      * discover similiar music
      * discover similiar movies
  
&nbsp;
      
#### **5.** What are the advantages and disadvantages of a very ﬂexible (versus a less ﬂexible) approach for regression or classiﬁcation? Under what circumstances might a more ﬂexible approach be preferred to a less ﬂexible approach? When might a less ﬂexible approach be preferred?

&nbsp;

  (a) The advantages of a flexible approach are that it reduces bias and fits non-linear models better. The disadvantages of a flexible approach are that it requires estimating a greater number of parameters and it will overfit at some point. Furthermore it increases the model variance.
A more flexible approach would be prefered if we are interested in prediction and not the interpretability of the results. A less flexible approach would be prefered if we are interested in inference and the interpretability of the results.

&nbsp;

#### **6.** Describe the diﬀerences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classiﬁcation (as opposed to a nonparametric approach)? What are its disadvantages?

&nbsp;

  (a) A parametric approach reduces the problem of estimating f down to one of estimating a set of parameters because it assumes a form of f. A non parametric approach does not asume a particular form of f but requires a very large sample size to accurately estimate f. 
  Advantages: simplifing the modelling of f down to one of estimating a set of parameters 
  Disadvantages: The model we chose will usually not match the true form of f –> estimate will be poor. We can avoid this by adding flexibilty. But the more flexible or model is the more it tends to overfit.

&nbsp; 
  
#### **7.** The table below provides a training data set containing six observations, three predictors, and one qualitative response variable. Suppose we wish to use this data set to make a prediction for Y when X1 = X2 = X3 = 0 using K-nearest neighbors.

&nbsp;

| Obs. | X~1~ | X~2~ | X~3~ | Y |
|:-:|:-:|:-:|:-:|:-:|
|1|0|3|0|Red|
|2|2|0|0|Red|
|3|0|1|3|Red|
|4|0|1|2|Green|
|5|-1|0|1|Green|
|6|1|1|1|Red|


  (a) Compute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0

```{r}
obs1 = c(0, 3, 0)
obs2 = c(2, 0, 0)
obs3 = c(0, 1, 3)
obs4 = c(0, 1, 2)
obs5 = c(-1, 0, 1)
obs6 = c(1, 1, 1)
obs0 = c(0, 0, 0)
sqrt(sum((obs1-obs0)^2)) 
sqrt(sum((obs2-obs0)^2)) 
sqrt(sum((obs3-obs0)^2))
sqrt(sum((obs4-obs0)^2)) 
sqrt(sum((obs5-obs0)^2)) 
sqrt(sum((obs6-obs0)^2)) 
```

  
  (b) What is our prediction with K = 1? Why?  
  *nearest point is obs5 –> prediction is green *
  
  (c) What is our prediction with K = 3? Why?  
  *nearest points are ob5, obs6, obs 2 –> prediction is red with 2/3 probability*
  
  (d) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why?  
  *When K becomes larger, the decision boundary becomes inflexible (almost linear). Therefore K should be small*

&nbsp;

### Applied

&nbsp;

#### 8. College Data



  (a) Use the read.csv() function to read the data into R. Call the loaded data “college”. Make sure that you have the directory set to the correct location for the data.

```{r}
library(ISLR)
data(College)
college = read.csv("College.csv")
```

  (b) Look at the data using the fix() function. You should notice that the first column is just the name of each university. We don’t really want R to treat this as data. However, it may be handy to have these names for later.

```{r}
rownames(college)=college[,1]
college =college [,-1] 
fix(college)
```

  (c) 
  i. Use the summary() function to produce a numerical summary of the variables in the data set.
```{r}
summary(college)
``` 
  
  ii. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data.
```{r}
pairs(college[, 1:10])
```  
  
  iii. Use the plot() function to produce side-by-side boxplots of “Outstate” versus “Private”. 
```{r}
plot(college$Private, college$Outstate, xlab = "Private University", ylab ="Out of State tuition in USD", main = "Outstate Tuition Plot", col=c('powderblue', 'mistyrose'))
```    
  
  iv. Create a new qualitative variable, called Elite, by binning the Top10perc variable .
```{r}  
Elite=rep("No",nrow(college ))
Elite[college$Top10perc >50]=" Yes"
Elite=as.factor(Elite)
college=data.frame(college ,Elite)
summary(college$Elite)
plot(college$Elite, college$Outstate, xlab = "Elite University", ylab ="Out of State tuition in USD", main = "Outstate Tuition Plot", col=c('powderblue', 'mistyrose'))
```

  v. Use the hist() function to produce some histograms with diﬀering numbers of bins for a few of the quantitative variables.
  
```{r}
par(mfrow = c(2,2))
hist(college$Apps, col = 5, xlab = "Accepted Applications", ylab = "Count")
hist(college$PhD, col = 2, xlab = "PhD", ylab = "Count")
hist(college$Grad.Rate, col = 3, xlab = "Grad Rate", ylab = "Count")
hist(college$Personal, col = 4, xlab = "Estimated Personal Spending", ylab = "Count")
```   
  
  vi. Continue exploring the data, and provide a brief summary of what you discover.
  
```{r}
summary(college$Grad.Rate)
```
  A graduation rate of 118% is quite weird.

&nbsp;
  
#### **9.** This exercise involves the “Auto” data set studied in the lab. Make sure the missing values have been removed from the data.



  (a) Which of the predictors are quantitative, and which are qualitative?
  
```{r, warning=FALSE, message=FALSE}
Auto = read.csv("Auto.csv", na.strings = "?")
Auto = na.omit(Auto)
str(Auto)
```
    All but origin and name are quantitative.

  (b) What is the range of each quantitative predictor?
```{r}
range(Auto$mpg)
range(Auto$cylinders)
range(Auto$displacement)
range(Auto$horsepower)
range(Auto$weight)
range(Auto$acceleration)
range(Auto$year)
```
  (c) What is the mean and standard deviation of each quantitative predictor?

```{r}
sapply(Auto[,1:7], mean)
sapply(Auto[,1:7], sd)
```

  (d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?
```{r}

Auto_dropped = Auto[-(10:85),-(8:9)]
sapply(Auto_dropped, range)
sapply(Auto_dropped, mean)
sapply(Auto_dropped, sd)
```
  (e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.

```{r}
pairs(Auto[,1:7])
```
```{r}
cor(Auto[,1:7], method = "pearson")
```
Some Variables are heavily correlated.

  (f) Suppose that we wish to predict gas mileage (“mpg”) on the basis of other variables. Do your plots suggest that any of the other variables might be useful in predicting “mpg”?
  
Yes, since almost every variable is at least moderately correlated with mpg.

&nbsp;

#### **10.** This exercise involves the “Boston” housing data set.

&nbsp;

  (a) To begin, load in the “Boston” data set.
```{r, warning=FALSE, message=FALSE}
require(MASS)
data(Boston)
str(Boston)
```
  (b) Make some pairwise scatterplots of the predictors in this data set.

```{r}
pairs(Boston)
```

  (c) Are any of the predictors associated with per capita crime rate?
  
```{r}
cor(Boston, method = "pearson")
```
The variables rad and tax have the strongest linear relationship with crime per capita.


  (d) Do any of the suburbs of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios?
```{r}
hist(Boston$crim, breaks = 25)
```


```{r}
nrow(  Boston[Boston$crim > 20, ])
```


```{r}
hist(Boston$tax, breaks = 25)
```

```{r}
hist(Boston$ptratio, breaks = 25)
```
All three variables have some outliers.


  (e) How many of the suburbs in this data set bound the Charles river?
  
```{r}
nrow(Boston[Boston$chas == 1, ])
```
  (f) What is the median pupil-teacher ratio among the towns in this data set?
```{r}

t(Boston[Boston$medv == min(Boston$medv),])
sapply(Boston, quantile)
```
  (g) Which suburb of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that suburb, and how do those values compare to the overall ranges for those predictors?

  (h) In this data set, how many of the suburbs average more than seven rooms per dwelling? More than eight rooms per dwelling?

```{r}
nrow(Boston[Boston$rm > 7,])
nrow(Boston[Boston$rm > 8,])
summary(subset(Boston, rm > 8))
summary(Boston)
```